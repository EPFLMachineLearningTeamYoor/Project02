{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from string import punctuation\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import gc\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation,Embedding,MaxPooling1D,LSTM,Bidirectional\n",
    "from keras.callbacks import EarlyStopping \n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.core import Dense, Dropout\n",
    "import keras.backend as K\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "#Dictionary size.top Max_word_count words\n",
    "Max_word_count=300000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use only smaller set for memory problems\n",
    "neg_twit_path='data/train_neg.txt'\n",
    "pos_twit_path='data/train_pos.txt'\n",
    "test_tweet_path='data/test_data.txt'\n",
    "test_tweet_only_path='data/test_tweet_data.txt'\n",
    "neg_twit_cleaned='cleaned twits/train_neg.txt'\n",
    "pos_twit_cleaned='cleaned twits/train_pos.txt'\n",
    "test_tweet_cleaned='cleaned twits/test_data.txt'\n",
    "gloveFile='Glove embeddings/glove.twitter.27B.50d.txt'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanTweets(tweets):\n",
    "    \n",
    "    remove_numbers_start = r'\\d+(\\w*|\\d*)'\n",
    "    remove_numbers_end = r'(\\w*|\\d*)\\d+'\n",
    "    remove_hashtags = r'#(\\w*|#*|\\d*)'\n",
    "    remove_underscores = r'_(\\w*)'\n",
    "    remove_punctuation = r\"[.,;'?():-_!$&%{}~/|]\"\n",
    "    remove_quotation = r'\"'\n",
    "    remove_math_ops = r'[-+.^:,*]'  \n",
    "    remove_delimeted_words = r'<[^>]+>'\n",
    "    \n",
    "    combined_pattern = r'|'.join((remove_numbers_start,remove_numbers_end,remove_hashtags,remove_underscores,remove_quotation,remove_delimeted_words))\n",
    "    combined_pattern2 = r'|'.join((remove_punctuation,remove_math_ops))\n",
    "    cleaned_tweets1 = re.sub(combined_pattern,'',tweets)\n",
    "    cleaned_tweets2 = re.sub(combined_pattern2,'',cleaned_tweets1)\n",
    "    print \"clean tweets: \",len(cleaned_tweets2)\n",
    "    return cleaned_tweets2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replaces all words having an instance of a character more than 2 timess\n",
    "def repl(matchObj):\t\n",
    "    char = matchObj.group(1)\n",
    "    return \"%s%s\" % (char, char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceRepeatingCharacters(tweets):\n",
    "    pattern = re.compile(r\"(\\w)\\1+\")\n",
    "    corrected_words = pattern.sub(repl,tweets)\n",
    "    return corrected_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(path,inputF, outputTweets):\n",
    "    print 'Loading raw data, cleaning and writing cleaned data...'\n",
    "\n",
    "    inputSet = path + inputF\n",
    "    outputSetA = outputTweets\n",
    "    \n",
    "    #read data sets\n",
    "    file = open(inputSet)\n",
    "    raw_data = file.read()\n",
    "    print \"raw_data:\",len(raw_data)\n",
    "\n",
    "    \n",
    "    cleaned_data = cleanTweets(raw_data)\n",
    "\n",
    "    strip_repeated_chars = replaceRepeatingCharacters(cleaned_data)\n",
    "    \n",
    "    \n",
    "    writeToFile(strip_repeated_chars,outputSetA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeToFile(data,outputSet):\n",
    "    print \"writing to file...\"\n",
    "    result = open(outputSet, 'w')\n",
    "    if (type(data) is list):\n",
    "        for item in data:\n",
    "            result.write(\"%s\\n\" % item)\n",
    "    elif (type(data) is str):\n",
    "        # remove whitespaces generated by data cleaning\n",
    "        newform = re.sub(' +',' ',data.lstrip())\n",
    "        result.write(newform)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load glove model\n",
    "def create_glove_model():\n",
    "    print \"Loading Glove Model...\"\n",
    "    f = open(gloveFile,'r')\n",
    "    glove_model = {}\n",
    "    for line in tqdm(f):\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        glove_model[word] = embedding\n",
    "    print \"Done.\",len(glove_model),\" words loaded!\"\n",
    "    return glove_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create tokenizer,create dictionary with sequences with same size\n",
    "#and Y values like [1.0 0.0] and [0.0 1.0]\n",
    "def tokenize(corpus,Y,max_tweet_lenght):\n",
    "    tknzr = Tokenizer(num_words=Max_word_count)\n",
    "    tknzr.fit_on_texts(corpus)\n",
    "    sqncs = tknzr.texts_to_sequences(corpus)\n",
    "    word_index = tknzr.word_index\n",
    "    X_processed = pad_sequences(sqncs, maxlen=max_tweet_lenght)\n",
    "    Y_processed = to_categorical(np.asarray(Y), 2)\n",
    "\n",
    "    return X_processed, Y_processed,word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load cleaned and written clean tweets before\n",
    "def load_clean_twits(filename):\n",
    "    print 'File reading and loading clean tweets...'\n",
    "    data = open(filename, 'rb')\n",
    "    clean_tweet_all = []\n",
    "    for tweet in tqdm(data):\n",
    "        clean_tweet_all.append(tweet)\n",
    "    return np.array(clean_tweet_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge positive and negative dataset\n",
    "def merge_dataset(pos_data, neg_data):\n",
    "    print 'Merging positive and negative dataset...'\n",
    "    X = np.concatenate((pos_data, neg_data))\n",
    "    y = np.array([1] * pos_data.shape[0] + [0] * neg_data.shape[0])\n",
    "    assert len(y) == X.shape[0]\n",
    "    assert X.shape[0] == pos_data.shape[0] + neg_data.shape[0]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create embeddings for word index items \n",
    "def define_embedding(glove_model,word_index,n_dim,max_tweet_length):\n",
    "    embeddings = np.zeros((len(word_index)+1, n_dim))\n",
    "    print len(word_index)\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_vec = glove_model[word]\n",
    "            embeddings[i] = embedding_vec\n",
    "\n",
    "        except KeyError:\n",
    "                continue\n",
    "                    \n",
    "    return embeddings\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw data, cleaning and writing cleaned data...\n",
      "raw_data: 8507760\n",
      "clean tweets:  7231862\n",
      "writing to file...\n",
      "Loading raw data, cleaning and writing cleaned data...\n",
      "raw_data: 6871220\n",
      "clean tweets:  5691794\n",
      "writing to file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "95467it [00:00, 2860963.71it/s]\n",
      "97354it [00:00, 2564434.06it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File reading and loading clean tweets...\n",
      "File reading and loading clean tweets...\n",
      "Merging positive and negative dataset...\n",
      "Loading Glove Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1193517it [00:15, 78710.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. 1193515  words loaded!\n",
      "82295\n"
     ]
    }
   ],
   "source": [
    "#tweakable parameters\n",
    "max_tweet_length = 80\n",
    "n_dim=50\n",
    "batch_size = 64\n",
    "nb_epochs = 15\n",
    "\n",
    "loadData('../',neg_twit_path,neg_twit_cleaned)\n",
    "loadData('../',pos_twit_path,pos_twit_cleaned)\n",
    "\n",
    "pos_clean_twits=load_clean_twits(pos_twit_cleaned)\n",
    "neg_clean_twits=load_clean_twits(neg_twit_cleaned)\n",
    "X, y = merge_dataset(pos_clean_twits,neg_clean_twits)\n",
    "\n",
    "glove_model=create_glove_model()\n",
    "\n",
    "X_p, Y_p,word_index=tokenize(X,y,max_tweet_length)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_p, Y_p, test_size=0.2, random_state=42)\n",
    "\n",
    "embeddings=define_embedding(glove_model,word_index,n_dim,max_tweet_length)\n",
    "\n",
    "#X_trim=trim_dataset(X,max_tweet_length,tfidf,glove_model)\n",
    "\n",
    "#calculate_max_avarage_lenght(X_trim)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print X_train.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#trainable is false because we use frozen glove vectors that already trained\n",
    "model.add(Embedding(len(word_index)+1, n_dim, weights=[embeddings], input_length=max_tweet_length, trainable=False))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "#model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "    # Compile the model\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 154256 samples, validate on 38565 samples\n",
      "Epoch 1/10\n",
      "154256/154256 [==============================] - 469s 3ms/step - loss: 0.4635 - acc: 0.7680 - val_loss: 0.4282 - val_acc: 0.7917\n",
      "Epoch 2/10\n",
      "154256/154256 [==============================] - 467s 3ms/step - loss: 0.4217 - acc: 0.7961 - val_loss: 0.4078 - val_acc: 0.8063\n",
      "Epoch 3/10\n",
      "154256/154256 [==============================] - 467s 3ms/step - loss: 0.4091 - acc: 0.8055 - val_loss: 0.4108 - val_acc: 0.8051\n",
      "Epoch 4/10\n",
      "154256/154256 [==============================] - 471s 3ms/step - loss: 0.4018 - acc: 0.8093 - val_loss: 0.3958 - val_acc: 0.8135\n",
      "Epoch 5/10\n",
      "154256/154256 [==============================] - 485s 3ms/step - loss: 0.3959 - acc: 0.8130 - val_loss: 0.3988 - val_acc: 0.8105\n",
      "Epoch 6/10\n",
      "154256/154256 [==============================] - 476s 3ms/step - loss: 0.3910 - acc: 0.8156 - val_loss: 0.3936 - val_acc: 0.8143\n",
      "Epoch 7/10\n",
      "154256/154256 [==============================] - 484s 3ms/step - loss: 0.3856 - acc: 0.8184 - val_loss: 0.3904 - val_acc: 0.8168\n",
      "Epoch 8/10\n",
      "154256/154256 [==============================] - 486s 3ms/step - loss: 0.3827 - acc: 0.8208 - val_loss: 0.3905 - val_acc: 0.8155\n",
      "Epoch 9/10\n",
      "154256/154256 [==============================] - 481s 3ms/step - loss: 0.3796 - acc: 0.8222 - val_loss: 0.3875 - val_acc: 0.8179\n",
      "Epoch 10/10\n",
      "154256/154256 [==============================] - 475s 3ms/step - loss: 0.3759 - acc: 0.8239 - val_loss: 0.3947 - val_acc: 0.8158\n",
      "38565/38565 [==============================] - 35s 902us/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=nb_epochs,\n",
    "          validation_data=(X_test, Y_test))\n",
    "\n",
    "score, acc = model.evaluate(X_test, Y_test,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
