{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from string import punctuation\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import gc\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation\n",
    "from keras.callbacks import EarlyStopping \n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.core import Dense, Dropout, Flatten,Reshape\n",
    "import keras.backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_twit_path='data/train_neg_full.txt'\n",
    "pos_twit_path='data/train_pos_full.txt'\n",
    "test_tweet_path='data/test_data.txt'\n",
    "test_tweet_only_path='data/test_tweet_data.txt'\n",
    "neg_twit_cleaned='cleaned twits/train_neg.txt'\n",
    "pos_twit_cleaned='cleaned twits/train_pos.txt'\n",
    "test_tweet_cleaned='cleaned twits/test_data.txt'\n",
    "gloveFile='Glove embeddings/glove.twitter.27B.50d.txt'\n",
    "\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanTweets(tweets):\n",
    "    \n",
    "    remove_numbers_start = r'\\d+(\\w*|\\d*)'\n",
    "    remove_numbers_end = r'(\\w*|\\d*)\\d+'\n",
    "    remove_hashtags = r'#(\\w*|#*|\\d*)'\n",
    "    remove_underscores = r'_(\\w*)'\n",
    "    remove_punctuation = r\"[.,;'?():-_!$&%{}~/|]\"\n",
    "    remove_quotation = r'\"'\n",
    "    remove_math_ops = r'[-+.^:,*]'  \n",
    "    remove_delimeted_words = r'<[^>]+>'\n",
    "    \n",
    "    combined_pattern = r'|'.join((remove_numbers_start,remove_numbers_end,remove_hashtags,remove_underscores,remove_quotation,remove_delimeted_words))\n",
    "    combined_pattern2 = r'|'.join((remove_punctuation,remove_math_ops))\n",
    "    cleaned_tweets1 = re.sub(combined_pattern,'',tweets)\n",
    "    cleaned_tweets2 = re.sub(combined_pattern2,'',cleaned_tweets1)\n",
    "    print \"clean tweets: \",len(cleaned_tweets2)\n",
    "    return cleaned_tweets2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replaces all words having an instance of a character more than 2 timess\n",
    "def repl(matchObj):\t\n",
    "    char = matchObj.group(1)\n",
    "    return \"%s%s\" % (char, char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceRepeatingCharacters(tweets):\n",
    "    pattern = re.compile(r\"(\\w)\\1+\")\n",
    "    corrected_words = pattern.sub(repl,tweets)\n",
    "    return corrected_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(path,inputF, outputTweets):\n",
    "    print 'Loading raw data, cleaning and writing cleaned data...'\n",
    "\n",
    "    inputSet = path + inputF\n",
    "    outputSetA = outputTweets\n",
    "    \n",
    "    #read data sets\n",
    "    file = open(inputSet)\n",
    "    raw_data = file.read()\n",
    "    print \"raw_data:\",len(raw_data)\n",
    "\n",
    "    \n",
    "    cleaned_data = cleanTweets(raw_data)\n",
    "\n",
    "    strip_repeated_chars = replaceRepeatingCharacters(cleaned_data)\n",
    "    \n",
    "    \n",
    "    writeToFile(strip_repeated_chars,outputSetA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeToFile(data,outputSet):\n",
    "    print \"writing to file...\"\n",
    "    result = open(outputSet, 'w')\n",
    "    if (type(data) is list):\n",
    "        for item in data:\n",
    "            result.write(\"%s\\n\" % item)\n",
    "    elif (type(data) is str):\n",
    "        # remove whitespaces generated by data cleaning\n",
    "        newform = re.sub(' +',' ',data.lstrip())\n",
    "        result.write(newform)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_glove_model():\n",
    "    print \"Loading Glove Model...\"\n",
    "    f = open(gloveFile,'r')\n",
    "    glove_model = {}\n",
    "    for line in tqdm(f):\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        glove_model[word] = embedding\n",
    "    print \"Done.\",len(glove_model),\" words loaded!\"\n",
    "    return glove_model;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(tweet):\n",
    "    try:\n",
    "        tweet = unicode(tweet.decode('utf-8').lower())\n",
    "        tokens = tknzr.tokenize(tweet)\n",
    "        return tokens\n",
    "    except:\n",
    "        return 'NC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_tokenized_twits(filename):\n",
    "    print 'File reading and tokenizing clean tweets...'\n",
    "    data = open(filename, 'rb')\n",
    "    tokenized_tweet_all = []\n",
    "    for tweet in tqdm(data):\n",
    "        tokenized_tweet=tokenize(tweet)\n",
    "        tokenized_tweet_all.append(tokenized_tweet)\n",
    "    return np.array(tokenized_tweet_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_dataset(X,max_tweet_lenght,tfidf,glove_model):\n",
    "    print 'Trim dataset with tweet lenght...'\n",
    "    X_trim=[]\n",
    "    for i,tweet in enumerate(X):\n",
    "        if(len(tweet)<=max_tweet_length):\n",
    "            X_trim.append(tweet)\n",
    "        else:\n",
    "            trimmed_tweet=select_most_important_words(tweet,max_tweet_length,tfidf,glove_model)\n",
    "            X_trim.append(trimmed_tweet)\n",
    "\n",
    "    \n",
    "    return np.array(X_trim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_tf_idf(corpus):\n",
    "    print 'building tf-idf matrix ...'\n",
    "    vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=5)\n",
    "    matrix = vectorizer.fit_transform([tweet for tweet in corpus])\n",
    "    tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "    print 'vocab size :', len(tfidf)\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_max_avarage_lenght(corpus):\n",
    "    print 'Calculating avarage and max tweet lenght...'\n",
    "    avg_length = 0.0\n",
    "    max_length = 0\n",
    "    for tweet in corpus:\n",
    "        if len(tweet) > max_length:\n",
    "            max_length = len(tweet)\n",
    "        avg_length += float(len(tweet))\n",
    "    \n",
    "    print('Average tweet length: {}'.format(avg_length / float(len(corpus))))\n",
    "    print('Max tweet length: {}'.format(max_length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dataset(pos_data, neg_data):\n",
    "    print 'Merging positive and negative dataset...'\n",
    "    X = np.concatenate((pos_data, neg_data))\n",
    "    y = np.array([1] * pos_data.shape[0] + [0] * neg_data.shape[0])\n",
    "    assert len(y) == X.shape[0]\n",
    "    assert X.shape[0] == pos_data.shape[0] + neg_data.shape[0]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_embeddings(X,y,max_tweet_length,n_dim,glove_model):\n",
    "    print \"Embedding dataset...\"\n",
    "    X_embedded = np.zeros((X.shape[0], max_tweet_length, n_dim), dtype=K.floatx())\n",
    "    Y_embedded = np.zeros((y.shape[0], 2), dtype=np.int32)\n",
    "    deficit=0\n",
    "    for k,tweet in tqdm(enumerate(X)):\n",
    "        for i, token in enumerate(tweet):\n",
    "            try:\n",
    "                X_embedded[k, i, :] = glove_model[token]\n",
    "                \n",
    "            except KeyError:\n",
    "                deficit+=1\n",
    "                continue\n",
    "        Y_embedded[k, :] = [1.0, 0.0] if y[k] == 0 else [0.0, 1.0]\n",
    "        \n",
    "    print \"deficit: \",deficit\n",
    "    return X_embedded, Y_embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select most important n (max_tweet_lenght) words in order to truncate longer tweets using\n",
    "#tf-idf structure\n",
    "def select_most_important_words(tweet, max_tweet_lenght,tfidf,glove_model):\n",
    "    \n",
    "    tfidf_val=[]\n",
    "    tweet=np.array(tweet)\n",
    "    \n",
    "    if (len(tweet)<=max_tweet_lenght):\n",
    "        return tweet\n",
    "    \n",
    "    else:\n",
    "        for token in tweet:\n",
    "            try:\n",
    "                if token not in glove_model:\n",
    "                    tfidf_val.append(-1)\n",
    "                    continue\n",
    "                    \n",
    "                tfidf_val.append(tfidf[token])\n",
    "                \n",
    "            except KeyError:\n",
    "                tfidf_val.append(-1)\n",
    "                continue\n",
    "    #get largest n values index            \n",
    "    idx=np.array(tfidf_val,dtype=np.int64).argsort()[-max_tweet_lenght:][::-1]\n",
    "    idx_f=idx.flatten()\n",
    "    \n",
    "    return tweet[idx_f]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not used this time but beneficial for one vector for tweet\n",
    "def find_vector_representation_of_tweets(tokenized_tweets,dimension,tfidf):\n",
    "    tweets_embeddings = np.zeros((len(tokenized_tweets), dimension))\n",
    "    vec = np.zeros(dimension).reshape((1, dimension))\n",
    "    count = 0.\n",
    "    deficit=0\n",
    "    for i, tokenized_tweet in enumerate(tokenized_tweets):\n",
    "        for word in tokenized_tweet:\n",
    "            try:\n",
    "                vec += glove_model[word].reshape((1, dimension)) * tfidf[word]\n",
    "                count += 1.\n",
    "            except KeyError:\n",
    "                deficit+=1\n",
    "                continue\n",
    "        if count != 0:\n",
    "            vec /= count\n",
    "            \n",
    "        tweets_embeddings[i, :] = vec\n",
    "    \n",
    "    print \"deficit: \",deficit\n",
    "    return tweets_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tweet_length = 50\n",
    "n_dim=50\n",
    "batch_size = 32\n",
    "nb_epochs = 10\n",
    "\n",
    "loadData('../',neg_twit_path,neg_twit_cleaned)\n",
    "loadData('../',pos_twit_path,pos_twit_cleaned)\n",
    "\n",
    "pos_tokenized_twits=file_to_tokenized_twits(pos_twit_cleaned)\n",
    "neg_tokenized_twits=file_to_tokenized_twits(neg_twit_cleaned)\n",
    "\n",
    "glove_model=create_glove_model()\n",
    "\n",
    "\n",
    "X, y = merge_dataset(pos_tokenized_twits,neg_tokenized_twits)\n",
    "\n",
    "tfidf=define_tf_idf(X)\n",
    "\n",
    "\n",
    "X_trim=trim_dataset(X,max_tweet_length,tfidf,glove_model)\n",
    "\n",
    "calculate_max_avarage_lenght(X_trim)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_trim, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print X_train.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#real data so large for ny memory so I split train and test sets into\n",
    "#split_count set.\n",
    "split_count=3\n",
    "train_idx = list(range(X_train.shape[0]))\n",
    "test_idx = list(range(X_test.shape[0]))\n",
    "\n",
    "s_train_idx=np.array_split(train_idx, split_count)\n",
    "s_test_idx=np.array_split(test_idx, split_count)\n",
    "\n",
    "model = Sequential()\n",
    "    \n",
    "model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same', input_shape=(max_tweet_length,n_dim)))\n",
    "model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same'))\n",
    "model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same'))\n",
    "model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv1D(32, kernel_size=2, activation='elu', padding='same'))\n",
    "model.add(Conv1D(32, kernel_size=2, activation='elu', padding='same'))\n",
    "model.add(Conv1D(32, kernel_size=2, activation='elu', padding='same'))\n",
    "model.add(Conv1D(32, kernel_size=2, activation='elu', padding='same'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256, activation='tanh'))\n",
    "model.add(Dense(256, activation='tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(lr=0.0001, decay=1e-6),\n",
    "                  metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train and test split_count times\n",
    "for i in range(split_count):\n",
    "    print \"iteration of data: \",i\n",
    "    current_X_train=X_train[s_train_idx[i]]\n",
    "    current_Y_train=Y_train[s_train_idx[i]]\n",
    "    current_X_test=X_train[s_test_idx[i]]\n",
    "    current_Y_test=Y_train[s_test_idx[i]]\n",
    "    embedded_X_train,embedded_Y_train=define_embeddings(current_X_train,current_Y_train,max_tweet_length,n_dim,glove_model)\n",
    "    embedded_X_test,embedded_Y_test=define_embeddings(current_X_test,current_Y_test,max_tweet_length,n_dim,glove_model)\n",
    "    # Fit the model\n",
    "    model.fit(embedded_X_train, embedded_Y_train,\n",
    "              batch_size=batch_size,\n",
    "              shuffle=True,\n",
    "              epochs=nb_epochs,\n",
    "              validation_data=(embedded_X_test, embedded_Y_test),\n",
    "    callbacks=[EarlyStopping(min_delta=0.00025, patience=2)])\n",
    "    \n",
    "    del current_X_train,current_Y_train,current_X_test,current_Y_test,embedded_X_train,embedded_X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read test id, tweet and write only tweets\n",
    "def read_write_test_data(base_dir,input_path,output_path):\n",
    "    print \"reading test data, seperating id and write only tweets\"\n",
    "    data = open(base_dir+input_path, 'rb')\n",
    "    result = open(base_dir+output_path, 'w')\n",
    "    idxes = []\n",
    "    for line in tqdm(data):\n",
    "        idx, line = line.strip().decode(\"utf-8\").split(',', 1)\n",
    "        idxes.append(idx)\n",
    "        result.write(\"%s\\n\" % line)\n",
    "    return idxes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create embeddings of test tweets and truncute longer tweets than max_tweet_lenght\n",
    "#input fotmat of cnn require this format [seq,max_lenght,emb_vec]\n",
    "def test_embeddings(corpus,max_tweet_length,n_dim,glove_model,tfidf):\n",
    "    print \"Embedding test data...\"\n",
    "    test_embedded = np.zeros((corpus.shape[0], max_tweet_length, n_dim), dtype=K.floatx())\n",
    "    \n",
    "    deficit=0\n",
    "    for k,tweet in tqdm(enumerate(corpus)):\n",
    "        trimmed_tweet=select_most_important_words(tweet,max_tweet_length,tfidf,glove_model)\n",
    "        for i, token in enumerate(trimmed_tweet):\n",
    "            try:\n",
    "                \n",
    "                test_embedded[k, i, :] = glove_model[token]\n",
    "                \n",
    "            except KeyError:\n",
    "                deficit+=1\n",
    "                continue\n",
    "                        \n",
    "    print \"deficit: \",deficit\n",
    "    return test_embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data (only tweet file)\n",
    "#Note that cleaning methods are disabled because these methods remove\n",
    "#some test tweets.possible bug\n",
    "def loadTestData(path,inputF, outputTweets):\n",
    "    print 'Loading raw data, cleaning and writing cleaned data...'\n",
    "\n",
    "    inputSet = path + inputF\n",
    "    outputSetA = outputTweets\n",
    "    \n",
    "    #read data sets\n",
    "    file = open(inputSet)\n",
    "    raw_data = file.read()\n",
    "    print \"raw_data:\",len(raw_data)\n",
    "\n",
    "    \n",
    "    #cleaned_data = cleanTweets(raw_data)\n",
    "\n",
    "    #strip_repeated_chars = replaceRepeatingCharacters(cleaned_data)\n",
    "    \n",
    "    \n",
    "    #writeToFile(strip_repeated_chars,outputSetA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load id, tweet file -->seperate tweets and write back-->load only tweets\n",
    "#-->tokenize tweets-->get embeddings of tweets-->predict tweets as whole\n",
    "def handle_test_data():\n",
    "    print \"handling test data and prediction...\"\n",
    "    f = open('submission_cnn.txt', 'w')\n",
    "    f.write(\"Id,Prediction\\n\")\n",
    "    results=0\n",
    "    idx=read_write_test_data(\"../\",test_tweet_path,test_tweet_only_path)\n",
    "    loadTestData('../',test_tweet_only_path,test_tweet_cleaned)\n",
    "    test_tokenized_twits=file_to_tokenized_twits(test_tweet_cleaned)\n",
    "    print \"tokenized:\",len(test_tweet_cleaned)\n",
    "\n",
    "    embedded_test_tweets=test_embeddings(test_tokenized_twits,max_tweet_length,n_dim,glove_model,tfidf)\n",
    "    prediction=model.predict(embedded_test_tweets)\n",
    "    for i,tweet in tqdm(enumerate(prediction)):\n",
    "        if(tweet[0]>tweet[1]):\n",
    "            results=-1\n",
    "        else:\n",
    "            results=1\n",
    "            \n",
    "        f.write(\"%s,%s\\n\" % (idx[i], results))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "handle_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
