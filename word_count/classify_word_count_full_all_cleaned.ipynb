{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import *\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "from sklearn.decomposition import PCA\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from operator import itemgetter\n",
    "from scipy.sparse import csc_matrix as smatrix\n",
    "import scipy\n",
    "from scipy.sparse import *\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "random_state = 42\n",
    "min_word_frequency = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all unique tokens in file\n",
    "def get_words(filename):\n",
    "    data = open(filename, 'rb')\n",
    "    res = []\n",
    "    for line in tqdm(data):\n",
    "        line = line.strip().decode(\"utf-8\").split(' ')\n",
    "        res += line\n",
    "    return list(set(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join two token lists to token -> id mapping\n",
    "def get_vocab(words_pos, words_neg):\n",
    "    all_words = list(set(words_pos + words_neg))\n",
    "    return {x: i for i, x in tqdm(enumerate(all_words))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file -> [[word_number_1_1, ..., word_number_1_K1], ..., [word_number_L_1, ..., word_number_L_KL]]\n",
    "def file_to_word2numbers(filename, vocab):\n",
    "    data = open(filename, 'rb')\n",
    "    word2numbers_all = []\n",
    "    for line in tqdm(data):\n",
    "        line = line.strip().decode(\"utf-8\").split(' ')\n",
    "        word2numbers = []\n",
    "        for word in line:\n",
    "            if word in vocab: word2numbers.append(vocab[word])\n",
    "        if word2numbers:\n",
    "            word2numbers_all.append(word2numbers)\n",
    "    return word2numbers_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of word occurences as embeddings (basic embeddings)\n",
    "def numbers_to_dataset(numbers, vocab):\n",
    "    arr = {}\n",
    "    for i, tweet in tqdm(enumerate(numbers)):\n",
    "        for number in tweet:\n",
    "            p = (i, number)\n",
    "            if p in arr: arr[p] += 1\n",
    "            else: arr[p] = 1\n",
    "                    \n",
    "    keys = list(arr.keys())\n",
    "    values = [arr[k] for k in keys]\n",
    "    return coo_matrix((values, ([x for x, y in keys], [y for x, y in keys])), shape=(len(numbers), len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing X, y pair\n",
    "def two_datasets_to_one(pos_data, neg_data):\n",
    "    assert pos_data.shape[1] == neg_data.shape[1]\n",
    "    X = scipy.sparse.vstack((pos_data, neg_data))\n",
    "    y = np.array([1] * pos_data.shape[0] + [0] * neg_data.shape[0])\n",
    "    assert len(y) == X.shape[0]\n",
    "    assert X.shape[0] == pos_data.shape[0] + neg_data.shape[0]\n",
    "    assert X.shape[1] == pos_data.shape[1]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns vector of token frequencies\n",
    "def get_word_count(fn, vocab):\n",
    "    data = open(fn, 'rb')\n",
    "    res = [0] * len(vocab)\n",
    "    for line in tqdm(data):\n",
    "        line = line.strip().decode(\"utf-8\").split(' ')\n",
    "        for w in line:\n",
    "            if w in vocab:\n",
    "                res[vocab[w]] += 1\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain dataset from two files using functions above\n",
    "def get_dataset(tweets_pos, tweets_neg, count_threshold = 100):\n",
    "    words_pos = get_words(tweets_pos)\n",
    "    words_neg = get_words(tweets_neg)\n",
    "    \n",
    "    vocab = get_vocab(words_pos, words_neg)\n",
    "    \n",
    "    # construct num -> word dict\n",
    "    reverse_dictionary = dict(zip(vocab.values(), vocab.keys()))\n",
    "    \n",
    "    # removing non-frequent words from vocab\n",
    "    word_count = get_word_count(tweets_pos, vocab) + get_word_count(tweets_neg, vocab)\n",
    "    use_words = [reverse_dictionary[i] for i, x in enumerate(word_count) if x > count_threshold]\n",
    "    print('Using %d words out of %d' % (len(use_words), len(vocab)))\n",
    "    vocab = {x: i for i, x in tqdm(enumerate(use_words))}\n",
    "    # construct num -> word dict\n",
    "    reverse_dictionary = dict(zip(vocab.values(), vocab.keys()))\n",
    "    \n",
    "    # loading data -> numbers of words\n",
    "    pos_numbers = file_to_word2numbers(tweets_pos, vocab)\n",
    "    neg_numbers = file_to_word2numbers(tweets_neg, vocab)\n",
    "    \n",
    "    # applying it to numbers\n",
    "    pos_data = numbers_to_dataset(pos_numbers, vocab)\n",
    "    neg_data = numbers_to_dataset(neg_numbers, vocab)\n",
    "    \n",
    "    # applying to datasets (pos & neg)\n",
    "    X, Y = two_datasets_to_one(pos_data, neg_data)\n",
    "    return vocab, reverse_dictionary, X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1126104it [00:02, 387291.22it/s]\n",
      "1117611it [00:02, 432756.07it/s]\n",
      "194282it [00:00, 887056.69it/s]\n",
      "1126104it [00:07, 159192.72it/s]\n",
      "1117611it [00:08, 130915.11it/s]\n",
      "12102it [00:00, 1254714.30it/s]\n",
      "6293it [00:00, 62900.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 12102 words out of 194282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1126104it [00:08, 126620.81it/s]\n",
      "1117611it [00:09, 116082.94it/s]\n",
      "1126052it [00:11, 101483.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# get full dataset\n",
    "vocab_full, rev_full, X_full, Y_full = get_dataset('../tmp/clean_train_pos.txt', '../tmp/clean_train_neg.txt', min_word_frequency)\n",
    "vocab, reverse_dictionary, X, Y = vocab_full, rev_full, X_full, Y_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get partial dataset\n",
    "#vocab_part, rev_part, X_part, Y_part = get_dataset('../data/train_pos.txt', '../data/train_neg.txt', min_word_frequency)\n",
    "#vocab, reverse_dictionary, X, Y = vocab_part, rev_part, X_part, Y_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot word length distribution\n",
    "def plot_word_count_hist(vocab):\n",
    "    lengths = [len(x) for x in vocab.keys()]\n",
    "    pd.DataFrame(lengths).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_word_count_hist(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split to train/val\n",
    "test_size_percent = 0.01\n",
    "x, x_val, y, y_val = train_test_split(X, Y, test_size=test_size_percent, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y).reshape(-1, 1)\n",
    "y_val = np.array(y_val).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X, y, batch_size, number_of_batches):\n",
    "    counter = 0\n",
    "    shuffle_index = np.arange(np.shape(y)[0])\n",
    "    np.random.shuffle(shuffle_index)\n",
    "    while 1:\n",
    "        index_batch = shuffle_index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X[index_batch,:].todense()\n",
    "        y_batch = y[index_batch]\n",
    "        counter += 1\n",
    "        yield(np.array(X_batch),y_batch)\n",
    "        if (counter >= number_of_batches):\n",
    "            np.random.shuffle(shuffle_index)\n",
    "            counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "hidden_layers = (200, 50, 20)\n",
    "reg_l2 = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "np.random.seed(random_state)\n",
    "\n",
    "kernel_regularizer = l2(reg_l2)\n",
    "\n",
    "# first hidden layer\n",
    "model.add(Dense(hidden_layers[0], input_dim=x.shape[1], activation='relu', kernel_regularizer = kernel_regularizer))\n",
    "\n",
    "# adding regularization\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# hidden layers\n",
    "for neurons_n in hidden_layers[1:]:\n",
    "    model.add(Dense(neurons_n, activation='relu', kernel_regularizer=kernel_regularizer))\n",
    "    model.add(Dropout(0.1))\n",
    "    \n",
    "#output layer\n",
    "model.add(Dense(1, activation='sigmoid', kernel_regularizer = kernel_regularizer))\n",
    "\n",
    "# showing accuracy\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# setting parameters\n",
    "batch_size = 10000\n",
    "nb_epoch = 10\n",
    "nb_batches = x.shape[0] / batch_size\n",
    "generator = batch_generator(x, y, batch_size, nb_batches)\n",
    "\n",
    "# training model\n",
    "model.fit_generator(generator = generator, epochs = nb_epoch, \n",
    "                    steps_per_epoch = nb_batches, validation_data=(x_val.toarray(), y_val))\n",
    "#validation_split=test_size_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_array_if_sparse(x):\n",
    "    if type(x) is scipy.sparse.csr.csr_matrix:\n",
    "        return x.toarray()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, x, sz = 500):\n",
    "    res = []\n",
    "    N = x.shape[0]\n",
    "    batches = N // sz\n",
    "    if batches * sz < N:\n",
    "        batches += 1\n",
    "    for batch in tqdm(range(batches)):\n",
    "        res += list(model.predict(to_array_if_sparse(x[batch * sz:(batch + 1) * sz,:])).flatten())\n",
    "    return np.array(res).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(model, x, y):\n",
    "    return np.mean((predict(model, x).flatten() > 0.5) == y.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print resulting train/val losses\n",
    "print('Accuracy on test: %.3f, on validation: %.3f' % (score(model, x, y), score(model, x_val, y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ROC curve\n",
    "def plot_ROC(x, y, model):\n",
    "    fpr, tpr, _ = roc_curve(y, predict(model, x))\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ROC(x_val, y_val, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score(model, x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open train and predict\n",
    "def test_to_dataset(filename):\n",
    "    data = open(filename, 'rb')\n",
    "    idxes = []\n",
    "    tweets_embeddings = []\n",
    "    \n",
    "    i = 1\n",
    "    for line in tqdm(data):\n",
    "        line = line.strip().decode(\"utf-8\")\n",
    "        idxes.append(i)\n",
    "        line = line.split(' ')\n",
    "        tweet = []\n",
    "        \n",
    "        tweet_embeddings = np.zeros((len(vocab), ), dtype=np.float32)\n",
    "        \n",
    "        for word in line:\n",
    "            if word in vocab:\n",
    "                tweet_embeddings[vocab[word]] += 1\n",
    "                \n",
    "        tweets_embeddings.append(tweet_embeddings)\n",
    "        i += 1\n",
    "        \n",
    "    #return tweets_embeddings\n",
    "    tweets_embeddings = np.array(tweets_embeddings)\n",
    "    assert len(idxes) == tweets_embeddings.shape[0]\n",
    "    assert tweets_embeddings.shape[1] == len(vocab)\n",
    "    return idxes, tweets_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write resulting clf predictions to output_filename\n",
    "# using filename as tweets input (test)\n",
    "def write_result(filename, clf, output_filename):\n",
    "    idx_test, X_test = test_to_dataset(filename)\n",
    "    y_predicted = np.array(2 * ((predict(clf, X_test) > 0.5) - 0.5), dtype=np.int64)\n",
    "    answers = sorted(zip(idx_test, y_predicted), key = lambda x: int(x[0]))\n",
    "    f = open(output_filename, 'w')\n",
    "    f.write(\"Id,Prediction\\n\")\n",
    "    for idx, ans in answers:\n",
    "        f.write(\"%s,%s\\n\" % (idx, ans))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_result('../data/test_data.txt', model, 'submission_count_full_nn_cleaned.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_test, X_test = test_to_dataset('../tmp/clean_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = predict(model, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
