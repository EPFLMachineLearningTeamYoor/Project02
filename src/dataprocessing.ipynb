{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import enchant\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.metrics import edit_distance\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOADING DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a method that takes raw text and generates raw text out of it/ op parameter is used to either \n",
    "#to keep redundant elements or remove them\n",
    "def tokenize_tweets(raw_tweets,op):\n",
    "    tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "    tokens = tknzr.tokenize(raw_tweets)\n",
    "    words = [w.lower() for w in tqdm(tokens)]\n",
    "    if op == \"vocab\":\n",
    "        vocab = set(words)\n",
    "    elif op == \"raw\":\n",
    "        vocab = words\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a method that reads a file and removes all non-ascii characters from it \n",
    "# returns raw_text,list [tweets]\n",
    "def loadFile(filename):\n",
    "    f = open(filename)\n",
    "    raw_tweets = f.read()\n",
    "    remove_nonunicode = re.sub(r'[^\\x00-\\x7F]',' ', raw_tweets)\n",
    "    content = remove_nonunicode.splitlines()\n",
    "    return remove_nonunicode,content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEMMING WORDS\n",
    "\n",
    "Word stemming is the process to find the stem of a word. To be more concrete as an example consider the following words eating,eats both are variations of the word eat. In our approach we know that tweets may include many repeated characters within one word, where people stress on their emotions. Thus we convert these words to words having only 2 repetions of these characters and we check if the reduced version is valid (found in dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a method that finds a group of repeated letters and replaces them by 2 instances of that letter\n",
    "def repl(matchObj):\t\n",
    "    char = matchObj.group(1)\n",
    "    return \"%s%s\" % (char, char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks if the word has repeated characters if true then we invoke the previous method\n",
    "def replaceRepeatingCharacters(tweets):\n",
    "    pattern = re.compile(r\"(\\w)\\1+\")\n",
    "    corrected_words = pattern.sub(repl,tweets)\n",
    "    return corrected_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if current token is already correct(found in dictionary) \n",
    "#if not then call above function\n",
    "# if word has changed => there os a sequence of repeating characters and hence we reccur\n",
    "# otherwise the word is returned\n",
    "def certifyToken(token):\n",
    "    if wordnet.synsets(token): #if dictionary knows the token we return it\n",
    "        return token\n",
    "    reduced_token = replaceRepeatingCharacters(token) # remove repeating characters if any\n",
    "    if reduced_token != token: # if the reduced word is different we may remove further\n",
    "        return certifyToken(reduced_token)\n",
    "    else:\n",
    "        return reduced_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CORRECTING MISSPELLED WORDS\n",
    "\n",
    "In this block, we focus on correcting misspelled words in tweets. This is achieved by using an english dictionary provided by enchant library, which checks if a word is correct or wrong.If wrong it provides us with suggestions for relevant replacement. We then compute the levensteihn distance between our token and the provided suggestion.\n",
    "The first method constructs a python dictionary mapping every levensteihn distance to a count denoting number of suggestions that lie that far from the word. \n",
    "The second method replaces all wrong words by their respective suggestions based on a threshold we set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a method that checks wether the token is known by the dictionary provided by enchant library\n",
    "# if yes returns it otherwise the dictionary provides a sey of suggestions \n",
    "# we compute levensthein distance to with the first suggestion if the distance is less than a certain threshold\n",
    "# then we return the suggestion otherwise we keep the token as is\n",
    "def correct_misspelled_word(word_dict,token,levenshtein_thresh):\n",
    "    if word_dict.check(token):\n",
    "        return token\n",
    "    else:\n",
    "        suggestions = word_dict.suggest(token)\n",
    "        if(len(suggestions)>0):\n",
    "            filtered_suggestions = list(filter(lambda x: x.isalpha(),suggestions))\n",
    "            if (len(filtered_suggestions)>0):\n",
    "                levenshtein_dist = edit_distance(token, filtered_suggestions[0])\n",
    "                if levenshtein_dist<= levenshtein_thresh :\n",
    "                         return filtered_suggestions[0]\n",
    "                else:\n",
    "                    return token\n",
    "            else:\n",
    "                levenshtein_dist = edit_distance(token, suggestions[0])\n",
    "                if levenshtein_dist<= levenshtein_thresh :\n",
    "                         return suggestions[0]\n",
    "                else:\n",
    "                    return token\n",
    "                \n",
    "        else:\n",
    "            return token "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CORRECTING AND STEMMING ALL TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this method is used to construct our vocabullary that we use to build our occurrence matrix\n",
    "# it finds the stem of every token\n",
    "def stemm_all_tokens(tokens):\n",
    "    correct_tweets = [certifyToken(token) for token in tqdm(tokens)]\n",
    "    return correct_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this method is used to construct our vocabullary that we use to build our occurrence matrix\n",
    "# it corrects every misspelled words\n",
    "def correct_all_tokens(word_dict,tokens,levenshtein_thresh):\n",
    "    correct_tweets = [correct_misspelled_word(word_dict,token,levenshtein_thresh) for token in tqdm(tokens)]\n",
    "    return correct_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this method uses a map to either stem or correct every token in a tweet\n",
    "def correct_tweet(text,op,levenshtein_thresh):\n",
    "    tokens = list(tokenize_tweets(text,\"raw\"))\n",
    "    word_dict = enchant.Dict(\"en_US\")\n",
    "    if op == \"spelling\":\n",
    "        correct_tokens = lambda token : correct_misspelled_word(word_dict,token,levenshtein_thresh)  if token.isalpha() else token\n",
    "    elif op ==\"stemming\":\n",
    "        correct_tokens = lambda token : certifyToken(token)  if token.isalpha() else token\n",
    "    corrected_tokens = [correct_tokens(token) for token in tqdm(tokens)]\n",
    "    corrected_tweet = ' '.join(corrected_tokens)\n",
    "    return corrected_tweet  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this method takes a list of tweets and either corrects or stems the tweets by using a map function\n",
    "def correct_all_tweets(tweets,op,levenshtein_thresh = 0):\n",
    "    correct_tweets = [correct_tweet(x,op,levenshtein_thresh) for x in tqdm(tweets)]\n",
    "    return correct_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STRIP INITIAL CHARACTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_token(token):\n",
    "    contains_chars = re.search('[a-zA-Z]', token)\n",
    "    if contains_chars is not None:\n",
    "        regex = re.compile('[^a-zA-Z]')\n",
    "        stripped = regex.sub('',token)\n",
    "        return stripped\n",
    "    else:\n",
    "        return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_vocab(tokens):\n",
    "    correct_tweets = [strip_token(token) for token in tqdm(tokens)]\n",
    "    return correct_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_tokens(text):\n",
    "    tokens = list(tokenize_tweets(text,\"raw\"))\n",
    "    corrected_tokens = [strip_token(token) for token in tqdm(tokens)]\n",
    "    corrected_tweet = ' '.join(corrected_tokens)\n",
    "    return corrected_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_text(tweets):\n",
    "    correct_tweets = [strip_tokens(x) for x in tqdm(tweets)]\n",
    "    return correct_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERATE CLEAN TWEETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeToFile(data,outputSet):\n",
    "    result = open(outputSet, 'w')\n",
    "    if (type(data) is np.ndarray or type(data) is list):\n",
    "        for item in data:\n",
    "            result.write(\"%s\\n\" % item)\n",
    "        result.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(pos_file, neg_file):\n",
    "    print('Loading files...')\n",
    "    positive_raw,positive_tweets = loadFile(pos_file)\n",
    "    negative_raw,negative_tweets = loadFile(neg_file)\n",
    "    \n",
    "    print('Tokenizing tweets...')\n",
    "    positive_vocab =  tokenize_tweets(positive_raw,\"vocab\")\n",
    "    negative_vocab =  tokenize_tweets(negative_raw,\"vocab\")\n",
    "    word_dict = enchant.Dict(\"en_US\")\n",
    "    \n",
    "    print('Stripping text...')\n",
    "    filtered_tweets_pos = strip_text(positive_tweets)\n",
    "    filtered_tweets_neg = strip_text(negative_tweets)\n",
    "    \n",
    "    print('Correcting vocabulary...')\n",
    "    filtered_vocab_pos = correct_vocab(positive_vocab)\n",
    "    filtered_vocab_neg = correct_vocab(negative_vocab)\n",
    "    \n",
    "    print('Computing unique tweets...')\n",
    "    unique_tweets_pos = np.unique(filtered_tweets_pos)\n",
    "    unique_tweets_neg = np.unique(filtered_tweets_neg)\n",
    "    \n",
    "    print('Correcting tweets...')\n",
    "    correct_tweets_pos = correct_all_tweets(unique_tweets_pos ,\"spelling\",5)\n",
    "    correct_tweets_neg = correct_all_tweets(unique_tweets_neg ,\"spelling\",5)\n",
    "    \n",
    "    print('Obtaining correct vocabulary...')\n",
    "    correct_tweet_vocab_pos = np.unique(correct_all_tokens(word_dict,filtered_vocab_pos,5))\n",
    "    correct_tweet_vocab_neg = np.unique(correct_all_tokens(word_dict,filtered_vocab_neg,5))\n",
    "   \n",
    "    print('Writing results to files...')\n",
    "    writeToFile(correct_tweets_pos,'../tmp/clean_train_pos.txt')\n",
    "    writeToFile(correct_tweet_vocab_pos,'../tmp/clean_train_vocab_pos.txt')\n",
    "    writeToFile(correct_tweets_neg,'../tmp/clean_train_neg.txt')\n",
    "    writeToFile(correct_tweet_vocab_neg,'../tmp/clean_train_vocab_neg.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files...\n",
      "Tokenizing tweets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 244555/1250000 [00:00<00:00, 2444514.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stripping text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250000/1250000 [00:00<00:00, 2622098.15it/s]\n",
      "100%|██████████| 1250000/1250000 [00:00<00:00, 2360406.22it/s]\n",
      " 11%|█         | 31827/297339 [00:00<00:00, 318160.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting vocabulary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297339/297339 [00:00<00:00, 321046.50it/s]\n",
      "100%|██████████| 405374/405374 [00:01<00:00, 337971.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing unique tweets...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unorderable types: function() < function()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-0ee6e8ad9d3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/train_pos_full.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'../data/train_neg_full.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-50-b7d02f508492>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(pos_file, neg_file)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Computing unique tweets...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0munique_tweets_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tweets_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0munique_tweets_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tweets_neg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Project02/venv/lib/python3.5/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0mar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_unique1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Invalid axis kwarg specified for unique'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Project02/venv/lib/python3.5/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0mflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0maux\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unorderable types: function() < function()"
     ]
    }
   ],
   "source": [
    "generate('../data/train_pos_full.txt', '../data/train_neg_full.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
