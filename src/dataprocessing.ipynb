{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import enchant\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.metrics import edit_distance\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOADING DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_file, neg_file = '../data/train_pos_full.txt', '../data/train_neg_full.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_tqdm = lambda x : x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a method that takes raw text and generates raw text out of it/ op parameter is used to either \n",
    "#to keep redundant elements or remove them\n",
    "def tokenize_tweets(raw_tweets,op,tqdm_=no_tqdm):\n",
    "    tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "    tokens = tknzr.tokenize(raw_tweets)\n",
    "    words = [w.lower() for w in tqdm_(tokens)]\n",
    "    if op == \"vocab\":\n",
    "        vocab = set(words)\n",
    "    elif op == \"raw\":\n",
    "        vocab = words\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a method that reads a file and removes all non-ascii characters from it \n",
    "# returns raw_text,list [tweets]\n",
    "def loadFile(filename):\n",
    "    f = open(filename)\n",
    "    raw_tweets = f.read()\n",
    "    remove_nonunicode = re.sub(r'[^\\x00-\\x7F]',' ', raw_tweets)\n",
    "    content = remove_nonunicode.splitlines()\n",
    "    return remove_nonunicode,content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEMMING WORDS\n",
    "\n",
    "Word stemming is the process to find the stem of a word. To be more concrete as an example consider the following words eating,eats both are variations of the word eat. In our approach we know that tweets may include many repeated characters within one word, where people stress on their emotions. Thus we convert these words to words having only 2 repetions of these characters and we check if the reduced version is valid (found in dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a method that finds a group of repeated letters and replaces them by 2 instances of that letter\n",
    "def repl(matchObj):\t\n",
    "    char = matchObj.group(1)\n",
    "    return \"%s%s\" % (char, char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks if the word has repeated characters if true then we invoke the previous method\n",
    "def replaceRepeatingCharacters(tweets):\n",
    "    pattern = re.compile(r\"(\\w)\\1+\")\n",
    "    corrected_words = pattern.sub(repl,tweets)\n",
    "    return corrected_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if current token is already correct(found in dictionary) \n",
    "#if not then call above function\n",
    "# if word has changed => there os a sequence of repeating characters and hence we reccur\n",
    "# otherwise the word is returned\n",
    "def certifyToken(token):\n",
    "    if wordnet.synsets(token): #if dictionary knows the token we return it\n",
    "        return token\n",
    "    reduced_token = replaceRepeatingCharacters(token) # remove repeating characters if any\n",
    "    if reduced_token != token: # if the reduced word is different we may remove further\n",
    "        return certifyToken(reduced_token)\n",
    "    else:\n",
    "        return reduced_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CORRECTING MISSPELLED WORDS\n",
    "\n",
    "In this block, we focus on correcting misspelled words in tweets. This is achieved by using an english dictionary provided by enchant library, which checks if a word is correct or wrong.If wrong it provides us with suggestions for relevant replacement. We then compute the levensteihn distance between our token and the provided suggestion.\n",
    "The first method constructs a python dictionary mapping every levensteihn distance to a count denoting number of suggestions that lie that far from the word. \n",
    "The second method replaces all wrong words by their respective suggestions based on a threshold we set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a method that checks wether the token is known by the dictionary provided by enchant library\n",
    "# if yes returns it otherwise the dictionary provides a sey of suggestions \n",
    "# we compute levensthein distance to with the first suggestion if the distance is less than a certain threshold\n",
    "# then we return the suggestion otherwise we keep the token as is\n",
    "def correct_misspelled_word(word_dict,token):\n",
    "    levenshtein_thresh = 5\n",
    "    if word_dict.check(token):\n",
    "        return token\n",
    "    else:\n",
    "        suggestions = word_dict.suggest(token)\n",
    "        if suggestions:\n",
    "            levenshtein_dist = edit_distance(token, suggestions[0])\n",
    "            if levenshtein_dist <=  levenshtein_thresh:\n",
    "                return suggestions[0]\n",
    "            else:\n",
    "                return token\n",
    "                \n",
    "        else:\n",
    "            return token "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CORRECTING AND STEMMING ALL TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this method is used to construct our vocabullary that we use to build our occurrence matrix\n",
    "# it finds the stem of every token\n",
    "def stemm_all_tokens(tokens,tqdm_=no_tqdm):\n",
    "    correct_tweets = [certifyToken(token) for token in tqdm_(tokens)]\n",
    "    return correct_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = enchant.Dict(\"en_US\")\n",
    "def correct_global(token):\n",
    "    global word_dict\n",
    "    return correct_misspelled_word(word_dict,token)\n",
    "\n",
    "# this method is used to construct our vocabullary that we use to build our occurrence matrix\n",
    "# it corrects every misspelled words\n",
    "def correct_all_tokens(word_dict_,tokens, pool):\n",
    "    return list(tqdm(pool.imap(correct_global, tokens), total=len(tokens)))\n",
    "    #r = pool.map(correct_global, tokens)\n",
    "    #return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this method uses cache to correct tweets\n",
    "# voc2corrvoc: token 'beingcreative' -> 'being creative'\n",
    "def correct_tweet_cached(text, voc2corrvoc):\n",
    "    tokens = tokenize_tweets(text, \"raw\")\n",
    "    return ' '.join([voc2corrvoc[t] if t in voc2corrvoc else t for t in tokens]).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this method uses a map to either stem or correct every token in a tweet\n",
    "def correct_tweet(text,op,tqdm_=no_tqdm):\n",
    "    tokens = list(tokenize_tweets(text,\"raw\"))\n",
    "    word_dict = enchant.Dict(\"en_US\")\n",
    "    if op == \"spelling\":\n",
    "        correct_tokens = lambda token : correct_misspelled_word(word_dict,token)  if token.isalpha() else token\n",
    "    elif op ==\"stemming\":\n",
    "        correct_tokens = lambda token : certifyToken(token)  if token.isalpha() else token\n",
    "    corrected_tokens = [correct_tokens(token) for token in tqdm_(tokens)]\n",
    "    corrected_tweet = ' '.join(corrected_tokens)\n",
    "    return corrected_tweet  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this method takes a list of tweets and either corrects or stems the tweets by using a map function\n",
    "def correct_all_tweets(tweets,op, tqdm_=no_tqdm):\n",
    "    correct_tweets = [correct_tweet(x,op) for x in tqdm_(tweets)]\n",
    "    return correct_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this method takes a list of tweets and corrects them using cache\n",
    "# voc2corrvoc: token 'beingcreative' -> 'being creative'\n",
    "def correct_all_tweets_cached(tweets, voc2corrvoc):\n",
    "    correct_tweets = [correct_tweet_cached(x, voc2corrvoc) for x in tqdm(tweets)]\n",
    "    return correct_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STRIP INITIAL CHARACTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_token(token):\n",
    "    contains_chars = re.search('[a-zA-Z]', token)\n",
    "    if contains_chars is not None:\n",
    "        regex = re.compile('[^a-zA-Z]')\n",
    "        stripped = regex.sub('',token)\n",
    "        return stripped\n",
    "    else:\n",
    "        return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_vocab(tokens, tqdm_=no_tqdm):\n",
    "    correct_tweets = [strip_token(token) for token in tqdm_(tokens)]\n",
    "    return correct_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_tokens(text, tqdm_=no_tqdm):\n",
    "    tokens = list(tokenize_tweets(text,\"raw\"))\n",
    "    corrected_tokens = [strip_token(token) for token in tqdm_(tokens)]\n",
    "    corrected_tweet = ' '.join(corrected_tokens)\n",
    "    return corrected_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_text(tweets,tqdm_=no_tqdm):\n",
    "    correct_tweets = [strip_tokens(x) for x in tqdm_(tweets)]\n",
    "    return correct_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERATE CLEAN TWEETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeToFile(data,outputSet):\n",
    "    result = open(outputSet, 'w')\n",
    "    if (type(data) is np.ndarray or type(data) is list):\n",
    "        for item in data:\n",
    "            result.write(\"%s\\n\" % item)\n",
    "        result.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "pool = multiprocessing.Pool(processes=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files...\n",
      "Tokenizing tweets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17920971/17920971 [00:06<00:00, 2567501.36it/s]\n",
      "100%|██████████| 21421695/21421695 [00:08<00:00, 2557127.72it/s]\n",
      "  0%|          | 1218/1250000 [00:00<01:42, 12173.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stripping text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250000/1250000 [01:45<00:00, 11902.86it/s]\n",
      "100%|██████████| 1250000/1250000 [02:06<00:00, 9906.33it/s]\n",
      " 11%|█         | 31567/297339 [00:00<00:00, 315561.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting vocabulary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297339/297339 [00:00<00:00, 323492.90it/s]\n",
      "100%|██████████| 405374/405374 [00:01<00:00, 332031.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing unique tweets...\n"
     ]
    }
   ],
   "source": [
    "print('Loading files...')\n",
    "positive_raw,positive_tweets = loadFile(pos_file)\n",
    "negative_raw,negative_tweets = loadFile(neg_file)\n",
    "\n",
    "print('Tokenizing tweets...')\n",
    "positive_vocab =  tokenize_tweets(positive_raw,\"vocab\",tqdm)\n",
    "negative_vocab =  tokenize_tweets(negative_raw,\"vocab\",tqdm)\n",
    "word_dict = enchant.Dict(\"en_US\")\n",
    "\n",
    "print('Stripping text...')\n",
    "filtered_tweets_pos = strip_text(positive_tweets,tqdm)\n",
    "filtered_tweets_neg = strip_text(negative_tweets,tqdm)\n",
    "\n",
    "print('Correcting vocabulary...')\n",
    "filtered_vocab_pos = correct_vocab(positive_vocab,tqdm)\n",
    "filtered_vocab_neg = correct_vocab(negative_vocab,tqdm)\n",
    "\n",
    "print('Computing unique tweets...')\n",
    "unique_tweets_pos = np.unique(filtered_tweets_pos)\n",
    "unique_tweets_neg = np.unique(filtered_tweets_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining correct vocabulary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512890/512890 [1:21:21<00:00, 105.06it/s]\n",
      " 29%|██▉       | 87426/297339 [00:00<00:00, 873771.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing correctors\n",
      "Splitting correct dictionary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297339/297339 [00:00<00:00, 840595.48it/s]\n",
      "100%|██████████| 405374/405374 [00:00<00:00, 704287.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining unique correct vocabulary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1179/1126104 [00:00<01:35, 11785.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting tweets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1126104/1126104 [01:18<00:00, 14279.14it/s]\n",
      "100%|██████████| 1117611/1117611 [01:29<00:00, 12439.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing results to files...\n"
     ]
    }
   ],
   "source": [
    "print('Obtaining correct vocabulary...')\n",
    "filtered_vocab = list(set(filtered_vocab_pos + filtered_vocab_neg))\n",
    "correct_tweet_vocab = correct_all_tokens(word_dict, filtered_vocab, pool)\n",
    "\n",
    "print('Initializing correctors')\n",
    "corrector = dict(zip(filtered_vocab, correct_tweet_vocab))\n",
    "\n",
    "print('Splitting correct dictionary')\n",
    "correct_tweet_vocab_pos = [corrector[w] for w in tqdm(filtered_vocab_pos)]\n",
    "correct_tweet_vocab_neg = [corrector[w] for w in tqdm(filtered_vocab_neg)]\n",
    "\n",
    "print('Obtaining unique correct vocabulary...')\n",
    "correct_tweet_vocab_pos_unique = np.unique(correct_tweet_vocab_pos)\n",
    "correct_tweet_vocab_neg_unique = np.unique(correct_tweet_vocab_neg)\n",
    "\n",
    "print('Correcting tweets...')\n",
    "correct_tweets_pos = correct_all_tweets_cached(unique_tweets_pos, corrector)\n",
    "correct_tweets_neg = correct_all_tweets_cached(unique_tweets_neg, corrector)\n",
    "\n",
    "print('Writing results to files...')\n",
    "writeToFile(correct_tweets_pos,'../tmp/clean_train_pos.txt')\n",
    "writeToFile(correct_tweet_vocab_pos_unique,'../tmp/clean_train_vocab_pos.txt')\n",
    "writeToFile(correct_tweets_neg,'../tmp/clean_train_neg.txt')\n",
    "writeToFile(correct_tweet_vocab_neg_unique,'../tmp/clean_train_vocab_neg.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tqdm.get_lock().locks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pool.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
