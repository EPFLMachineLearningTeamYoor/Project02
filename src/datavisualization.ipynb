{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pylab as pl\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Vocabullary\n",
    "\n",
    "A function that reads vocabulary created by cleaning the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadVocab(fileName):\n",
    "\n",
    "# load file into list    \n",
    "    with open(fileName) as f:\n",
    "        content = f.readlines()\n",
    "        content = [x.strip() for x in content] \n",
    "        return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Vector\n",
    "\n",
    "This is a matrix that contains for each tweet the count of words in the vocabulary set that occured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createCountVector(fileName1,fileName2):\n",
    "\n",
    "    vocab_pos = loadVocab(fileName1)\n",
    "    vocab_neg = loadVocab(fileName2)\n",
    "    # find the combination of both vocabularies and remove common words\n",
    "    combined_vocab = np.unique(vocab_pos + vocab_neg)\n",
    "    # load the tweets \n",
    "    raw_text1 = pd.read_table('../data/train_pos.txt',header = None,names=['tweets'])\n",
    "    raw_text2 = pd.read_table('../data/train_neg.txt',header = None,names=['tweets'])\n",
    "    positiveTweets = raw_text1['tweets'].apply(str)\n",
    "    negativeTweets = raw_text2['tweets'].apply(str)\n",
    "    dataset = pd.concat([positiveTweets,negativeTweets])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # labels/output map : positive-> 0 and negative -> 1\n",
    "    labels = np.zeros((positiveTweets.shape[0] + negativeTweets.shape[0],),dtype=int)\n",
    "    labels[negativeTweets.shape[0]:] = 1\n",
    "    \n",
    "    #occurence matrix representation\n",
    "    cv = CountVectorizer(vocabulary=combined_vocab)\n",
    "    occurence = cv.fit_transform(dataset)\n",
    "    \n",
    "    \n",
    "\n",
    "    #frequency of word matrix representation\n",
    "    #tf_transformer = TfidfTransformer(use_idf=False).fit(occurence)\n",
    "    #frequency = tf_transformer.transform(occurence)\n",
    "    \n",
    "    \n",
    "    return occurence,labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VISUALIZATION\n",
    "\n",
    "Visualizing counts of words in tweets against labels using PCA and K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_vis(X,labels):\n",
    "\n",
    "    pca = TruncatedSVD(n_components=2)\n",
    "    projected = pca.fit_transform(X)\n",
    "    kmeans = KMeans(n_clusters=2, random_state=0).fit(projected)\n",
    "    first_cluster_index = np.where(kmeans.labels_ == 0)[0]\n",
    "    second_cluster_index = np.where(kmeans.labels_ == 1)[0]\n",
    "    clusterA = projected[first_cluster]\n",
    "    clusterB = projected[second_cluster]\n",
    "    c1 = pl.scatter(clusterA[:,0],clusterA[:,1],c='r',marker='+')\n",
    "    c2 = pl.scatter(clusterB[:,0],clusterB[:,1],c='g',marker='o')\n",
    "    pl.legend([c1, c2], ['ClusterA', 'ClusterB'])\n",
    "    pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embeddings_vis(embeddings,labels):\n",
    "    \n",
    "    sentence_embed = generate_sentences(embeddings)\n",
    "    kmeans = KMeans(n_clusters=2, random_state=0).fit(projected)\n",
    "    first_cluster_index = np.where(kmeans.labels_ == 0)[0]\n",
    "    second_cluster_index = np.where(kmeans.labels_ == 1)[0]\n",
    "    clusterA = projected[first_cluster]\n",
    "    clusterB = projected[second_cluster]\n",
    "    c1 = pl.scatter(clusterA[:,0],clusterA[:,1],c='r',marker='+')\n",
    "    c2 = pl.scatter(clusterB[:,0],clusterB[:,1],c='g',marker='o')\n",
    "    pl.legend([c1, c2], ['ClusterA', 'ClusterB'])\n",
    "    pl.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# PREDICTION \n",
    "\n",
    "First module : normalize count vector and perform pca on it (resulting in uncorrelated features) then feed it to a linear model(logistic regression) , non-linear model(svm) \n",
    "\n",
    "Second module: apply all previously mentioned models on the full matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count,labels = createCountVector(\"../vocabulary/train_pos_vocab.txt\",\"../vocabulary/train_neg_vocab.txt\")\n",
    "#visualization(count,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed = np.load('../sample_code/embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(196970, 85249)\n",
      "(21161, 20)\n",
      "(21161, 21161)\n"
     ]
    }
   ],
   "source": [
    "print(count.shape)\n",
    "print(embed.shape)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(a):\n",
    "    \n",
    "    col_sum = np.array(a.sum(axis=0).squeeze())[0]\n",
    "    col_nonzero = a.getnnz(axis=0)\n",
    "    col_nonzero[np.where(col_nonzero ==0)] = 1\n",
    "    col_avg = col_sum/col_nonzero\n",
    "    diagonal_matrix = sp.diags(col_avg, 0)\n",
    "    b = a.copy()\n",
    "    b.data = np.ones_like(b.data)\n",
    "    normalized_matrix = a - b*diagonal_matrix\n",
    "    return normalized_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test_split(data,labels):\n",
    "    \n",
    "    indices = np.arange(0,data.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    data_split = int(0.8 * indices.shape[0])\n",
    "    train_index = indices[:data_split]\n",
    "    test_index = indices[data_split:]\n",
    "\n",
    "    train_data = data[train_index,:]\n",
    "    train_label = labels[train_index]\n",
    "    test_data =  data[test_index,:]\n",
    "    test_label =  labels[test_index] \n",
    "    \n",
    "    return train_data,train_label,test_data,test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prediction(data,modeltype):\n",
    "    \n",
    "    res = normalize(data)\n",
    "       \n",
    "    for component in range(10,100,10):\n",
    "        pca = TruncatedSVD(n_components=component)\n",
    "        projected = pca.fit_transform(res)\n",
    "        train_data,train_label,test_data,test_label = train_test_split(projected,labels)\n",
    "        if modeltype == \"linear\":\n",
    "            logistic = LogisticRegression(C=1e5)\n",
    "            model = logistic.fit(train_data,train_label)\n",
    "            Y_pred = model.predict(test_data)\n",
    "            \n",
    "        elif modeltype == \"nonlinear\":    \n",
    "        \n",
    "            rbf_svc = svm.SVC(kernel='rbf')\n",
    "            model = rbf_svc.fit(train_data, train_label) \n",
    "            Y_pred = model.predict(test_data)\n",
    "        \n",
    "        accuracy = accuracy_score(test_label,Y_pred)\n",
    "        print(str(component) + \" \" + str(accuracy))           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21161, 21161)\n"
     ]
    }
   ],
   "source": [
    "prediction(count)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.774128039803\n"
     ]
    }
   ],
   "source": [
    "train_data,train_label,test_data,test_label = train_test_split(count,labels)\n",
    "logistic = LogisticRegression(C=1e5)\n",
    "model = logistic.fit(train_data,train_label)\n",
    "Y_pred = model.predict(test_data)\n",
    "accuracy = accuracy_score(test_label,Y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data,train_label,test_data,test_label = train_test_split(count,labels)\n",
    "rbf_svc = svm.SVC(kernel='rbf')\n",
    "model = rbf_svc.fit(train_data, train_label) \n",
    "Y_pred = model.predict(test_data)\n",
    "accuracy = accuracy_score(test_label,Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
