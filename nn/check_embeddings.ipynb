{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "from string import punctuation\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import gc\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation,Embedding,MaxPooling1D,LSTM,Bidirectional\n",
    "from keras.callbacks import EarlyStopping \n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.core import Dense, Dropout\n",
    "import keras.backend as K\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "#Dictionary size.top Max_word_count words\n",
    "Max_word_count=300000\n",
    "\n",
    "max_tweet_length = 80\n",
    "n_dim=100\n",
    "batch_size = 64\n",
    "nb_epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use only smaller set for memory problems\n",
    "neg_twit_path='data/train_neg.txt'\n",
    "pos_twit_path='data/train_pos.txt'\n",
    "test_tweet_path='data/test_data.txt'\n",
    "test_tweet_only_path='data/test_tweet_data.txt'\n",
    "neg_twit_cleaned='data/clean/train_neg.txt'\n",
    "pos_twit_cleaned='data/clean/train_pos.txt'\n",
    "test_tweet_cleaned='data/clean/test_data.txt'\n",
    "gloveFile='glove.twitter.27B.100d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanTweets(tweets):\n",
    "    \n",
    "    remove_numbers_start = r'\\d+(\\w*|\\d*)'\n",
    "    remove_numbers_end = r'(\\w*|\\d*)\\d+'\n",
    "    remove_hashtags = r'#(\\w*|#*|\\d*)'\n",
    "    remove_underscores = r'_(\\w*)'\n",
    "    remove_punctuation = r\"[.,;'?():-_!$&%{}~/|]\"\n",
    "    remove_quotation = r'\"'\n",
    "    remove_math_ops = r'[-+.^:,*]'  \n",
    "    remove_delimeted_words = r'<[^>]+>'\n",
    "    \n",
    "    combined_pattern = r'|'.join((remove_numbers_start,remove_numbers_end,remove_hashtags,remove_underscores,remove_quotation,remove_delimeted_words))\n",
    "    combined_pattern2 = r'|'.join((remove_punctuation,remove_math_ops))\n",
    "    cleaned_tweets1 = re.sub(combined_pattern,'',tweets)\n",
    "    cleaned_tweets2 = re.sub(combined_pattern2,'',cleaned_tweets1)\n",
    "    print(\"clean tweets: \" + str(len(cleaned_tweets2)))\n",
    "    return cleaned_tweets2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replaces all words having an instance of a character more than 2 timess\n",
    "def repl(matchObj):\t\n",
    "    char = matchObj.group(1)\n",
    "    return \"%s%s\" % (char, char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceRepeatingCharacters(tweets):\n",
    "    pattern = re.compile(r\"(\\w)\\1+\")\n",
    "    corrected_words = pattern.sub(repl,tweets)\n",
    "    return corrected_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(path,inputF, outputTweets):\n",
    "    print('Loading raw data, cleaning and writing cleaned data...')\n",
    "\n",
    "    inputSet = path + inputF\n",
    "    outputSetA = outputTweets\n",
    "    \n",
    "    #read data sets\n",
    "    file = open(inputSet)\n",
    "    raw_data = file.read()\n",
    "    print(\"raw_data:\" + str(len(raw_data)))\n",
    "\n",
    "    \n",
    "    cleaned_data = cleanTweets(raw_data)\n",
    "\n",
    "    strip_repeated_chars = replaceRepeatingCharacters(cleaned_data)\n",
    "    \n",
    "    \n",
    "    writeToFile(strip_repeated_chars,outputSetA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeToFile(data,outputSet):\n",
    "    print(\"writing to file...\")\n",
    "    result = open(outputSet, 'w')\n",
    "    if (type(data) is list):\n",
    "        for item in data:\n",
    "            result.write(\"%s\\n\" % item)\n",
    "    elif (type(data) is str):\n",
    "        # remove whitespaces generated by data cleaning\n",
    "        newform = re.sub(' +',' ',data.lstrip())\n",
    "        result.write(newform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load glove model\n",
    "def create_glove_model():\n",
    "    print(\"Loading Glove Model...\")\n",
    "    f = open(gloveFile,'r')\n",
    "    glove_model = {}\n",
    "    for line in tqdm(f):\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        glove_model[word] = embedding\n",
    "    print(\"Done.\" + str(len(glove_model)) + \" words loaded!\")\n",
    "    return glove_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create tokenizer,create dictionary with sequences with same size\n",
    "#and Y values like [1.0 0.0] and [0.0 1.0]\n",
    "def tokenize(corpus,Y,max_tweet_lenght):\n",
    "    corpus_str = [x.decode('utf-8') for x in X]\n",
    "    tknzr = Tokenizer(num_words=Max_word_count)\n",
    "    tknzr.fit_on_texts(corpus_str)\n",
    "    sqncs = tknzr.texts_to_sequences(corpus_str)\n",
    "    word_index = tknzr.word_index\n",
    "    X_processed = pad_sequences(sqncs, maxlen=max_tweet_lenght)\n",
    "    Y_processed = to_categorical(np.asarray(Y), 2)\n",
    "\n",
    "    return X_processed, Y_processed,word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load cleaned and written clean tweets before\n",
    "def load_clean_twits(filename):\n",
    "    print('File reading and loading clean tweets...')\n",
    "    data = open(filename, 'rb')\n",
    "    clean_tweet_all = []\n",
    "    for tweet in tqdm(data):\n",
    "        clean_tweet_all.append(tweet)\n",
    "    return np.array(clean_tweet_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge positive and negative dataset\n",
    "def merge_dataset(pos_data, neg_data):\n",
    "    print('Merging positive and negative dataset...')\n",
    "    X = np.concatenate((pos_data, neg_data))\n",
    "    y = np.array([1] * pos_data.shape[0] + [0] * neg_data.shape[0])\n",
    "    assert len(y) == X.shape[0]\n",
    "    assert X.shape[0] == pos_data.shape[0] + neg_data.shape[0]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create embeddings for word index items \n",
    "def define_embedding(glove_model,word_index,n_dim,max_tweet_length):\n",
    "    embeddings = np.zeros((len(word_index)+1, n_dim))\n",
    "    print(str(len(word_index)))\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_vec = glove_model[word]\n",
    "            embeddings[i] = embedding_vec\n",
    "\n",
    "        except KeyError:\n",
    "                continue\n",
    "                    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw data, cleaning and writing cleaned data...\n",
      "raw_data:8507752\n",
      "clean tweets: 7231854\n",
      "writing to file...\n",
      "Loading raw data, cleaning and writing cleaned data...\n",
      "raw_data:6871220\n",
      "clean tweets: 5691794\n",
      "writing to file...\n"
     ]
    }
   ],
   "source": [
    "loadData('../',neg_twit_path,neg_twit_cleaned)\n",
    "loadData('../',pos_twit_path,pos_twit_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "95467it [00:00, 1626226.50it/s]\n",
      "97354it [00:00, 1762613.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File reading and loading clean tweets...\n",
      "File reading and loading clean tweets...\n",
      "Merging positive and negative dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2854it [00:00, 28524.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1193517it [00:50, 23430.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.1193515 words loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pos_clean_twits=load_clean_twits(pos_twit_cleaned)\n",
    "neg_clean_twits=load_clean_twits(neg_twit_cleaned)\n",
    "X, y = merge_dataset(pos_clean_twits,neg_clean_twits)\n",
    "\n",
    "glove_model=create_glove_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_p, Y_p, word_index = tokenize(X, y, max_tweet_length)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_p, Y_p, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82295\n"
     ]
    }
   ],
   "source": [
    "embeddings=define_embedding(glove_model,word_index,n_dim,max_tweet_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_index = {v: k for k, v in word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_words(word, n_closest = 10):\n",
    "    closest_words = np.argsort(np.linalg.norm(embeddings - embeddings[word_index[word], :], axis=1))\n",
    "    res = []\n",
    "    for w in closest_words[1:n_closest + 1]:\n",
    "        res.append(reverse_word_index[w])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failure: ['success', 'disappointment', 'mistake', 'progress', 'fear', 'failures', 'purpose', 'failing', 'surely', 'responsibility']\n",
      "success: ['progress', 'failure', 'successful', 'achieve', 'greatness', 'happiness', 'experience', 'opportunity', 'future', 'purpose']\n",
      "achievement: ['received', 'earned', 'achievements', 'collector', 'task', 'manor', 'novice', 'destroyer', 'completed', 'spokesperson']\n",
      "bad: ['way', 'crazy', 'mad', 'thing', 'sick', 'either', 'reason', 'stupid', 'cause', 'hell']\n",
      "good: ['great', 'well', 'better', 'nice', 'way', 'too', 'night', 'hope', 'morning', 'bad']\n",
      "god: ['lord', 'jesus', 'christ', 'bless', 'blessing', 'true', 'praise', 'gods', 'heaven', 'thank']\n",
      "dog: ['cat', 'dogs', 'puppy', 'pet', 'cats', 'horse', 'monkey', 'bear', 'kitten', 'pig']\n",
      "bonjour: ['coucou', 'nuit', 'monsieur', 'mademoiselle', 'salut', 'bonne', 'voila', 'twittos', 'maman', 'madame']\n",
      "nice: ['good', 'cool', 'pretty', 'well', 'lovely', 'awesome', 'sure', 'great', 'fun', 'look']\n",
      "king: ['prince', 'aka', 'queen', 'jack', 'legend', 'mr', 'kings', 'tony', 'royal', 'billy']\n",
      "sad: ['bad', 'sick', 'sigh', 'feel', 'upset', 'confused', 'disappointed', 'cry', 'depressed', 'reason']\n",
      "happy: ['birthday', 'day', 'bday', 'merry', 'wish', 'love', 'hope', 'enjoy', 'thank', 'wishes']\n"
     ]
    }
   ],
   "source": [
    "words = ['failure', 'success', 'achievement', 'bad', 'good', 'god', 'dog', 'bonjour', 'nice', 'king', 'sad', 'happy']\n",
    "for w in words:\n",
    "    print(w + \": \" + str(get_closest_words(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
