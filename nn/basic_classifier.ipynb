{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # provide sql-like data manipulation tools. very handy.\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np # high dimensional vector computing library.\n",
    "from copy import deepcopy\n",
    "from string import punctuation\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec # the word2vec model gensim class\n",
    "LabeledSentence = gensim.models.doc2vec.LabeledSentence # we'll talk about this down below\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import scale\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import gc\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation\n",
    "from keras.callbacks import EarlyStopping \n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "import keras.backend as K\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_twit_path='data/train_neg_full.txt'\n",
    "pos_twit_path='data/train_pos_full.txt'\n",
    "neg_twit_cleaned='cleaned twits/train_neg.txt'\n",
    "pos_twit_cleaned='cleaned twits/train_pos.txt'\n",
    "gloveFile='Glove embeddings/glove.twitter.27B.100d.txt'\n",
    "\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanTweets(tweets):\n",
    "    \n",
    "    remove_numbers_start = r'\\d+(\\w*|\\d*)'\n",
    "    remove_numbers_end = r'(\\w*|\\d*)\\d+'\n",
    "    remove_hashtags = r'#(\\w*|#*|\\d*)'\n",
    "    remove_underscores = r'_(\\w*)'\n",
    "    remove_punctuation = r\"[.,;'?():-_!$&%{}~/|]\"\n",
    "    remove_quotation = r'\"'\n",
    "    remove_math_ops = r'[-+.^:,*]'  \n",
    "    remove_delimeted_words = r'<[^>]+>'\n",
    "    \n",
    "    combined_pattern = r'|'.join((remove_numbers_start,remove_numbers_end,remove_hashtags,remove_underscores,remove_quotation,remove_delimeted_words))\n",
    "    combined_pattern2 = r'|'.join((remove_punctuation,remove_math_ops))\n",
    "    cleaned_tweets1 = re.sub(combined_pattern,'',tweets)\n",
    "    cleaned_tweets2 = re.sub(combined_pattern2,'',cleaned_tweets1)\n",
    "    return cleaned_tweets2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replaces all words having an instance of a character more than 2 timess\n",
    "def repl(matchObj):\t\n",
    "    char = matchObj.group(1)\n",
    "    return \"%s%s\" % (char, char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceRepeatingCharacters(tweets):\n",
    "    pattern = re.compile(r\"(\\w)\\1+\")\n",
    "    corrected_words = pattern.sub(repl,tweets)\n",
    "    return corrected_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(path,inputF, outputTweets):\n",
    "    \n",
    "    inputSet = path + inputF\n",
    "    outputSetA = outputTweets\n",
    "    \n",
    "    #read data sets\n",
    "    file = open(inputSet)\n",
    "    raw_data = file.read()\n",
    "    \n",
    "    cleaned_data = cleanTweets(raw_data)\n",
    "    strip_repeated_chars = replaceRepeatingCharacters(cleaned_data)\n",
    "    \n",
    "    writeToFile(strip_repeated_chars,outputSetA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeToFile(data,outputSet):\n",
    "    result = open(outputSet, 'w')\n",
    "    if (type(data) is list):\n",
    "        for item in data:\n",
    "            result.write(\"%s\\n\" % item)\n",
    "    elif (type(data) is str):\n",
    "        # remove whitespaces generated by data cleaning\n",
    "        newform = re.sub(' +',' ',data.lstrip())\n",
    "        result.write(newform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadData('../',neg_twit_path,neg_twit_cleaned)\n",
    "loadData('../',pos_twit_path,pos_twit_cleaned)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"Loading Glove Model\"\n",
    "f = open(gloveFile,'r')\n",
    "glove_model = {}\n",
    "for line in tqdm(f):\n",
    "    splitLine = line.split()\n",
    "    word = splitLine[0]\n",
    "    embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "    glove_model[word] = embedding\n",
    "print \"Done.\",len(glove_model),\" words loaded!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(tweet):\n",
    "    try:\n",
    "        tweet = unicode(tweet.decode('utf-8').lower())\n",
    "        tokens = tknzr.tokenize(tweet)\n",
    "        return tokens\n",
    "    except:\n",
    "        return 'NC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_tokenized_twits(filename):\n",
    "    data = open(filename, 'rb')\n",
    "    tokenized_tweet_all = []\n",
    "    for tweet in tqdm(data):\n",
    "        tokenized_tweet=tokenize(tweet)\n",
    "        tokenized_tweet_all.append(tokenized_tweet)\n",
    "    return np.array(tokenized_tweet_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pos_tokenized_twits=file_to_tokenized_twits(pos_twit_cleaned)\n",
    "neg_tokenized_twits=file_to_tokenized_twits(neg_twit_cleaned)\n",
    "del pos_twit_cleaned\n",
    "del neg_twit_cleaned\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print pos_tokenized_twits[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dataset(pos_data, neg_data):\n",
    "    X = np.concatenate((pos_data, neg_data))\n",
    "    y = np.array([1] * pos_data.shape[0] + [0] * neg_data.shape[0])\n",
    "    assert len(y) == X.shape[0]\n",
    "    assert X.shape[0] == pos_data.shape[0] + neg_data.shape[0]\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = merge_dataset(pos_tokenized_twits,neg_tokenized_twits)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print X[1:1191748]\n",
    "print y[1:1191748]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print X_train.shape, X_test.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_tf_idf(corpus):\n",
    "    print 'building tf-idf matrix ...'\n",
    "    vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=1)\n",
    "    matrix = vectorizer.fit_transform([tweet for tweet in corpus])\n",
    "    tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "    print 'vocab size :', len(tfidf)\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf=define_tf_idf(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_vector_representation_of_tweets(tokenized_tweets,dimension,tfidf):\n",
    "    tweets_embeddings = np.zeros((len(tokenized_tweets), dimension))\n",
    "    vec = np.zeros(dimension).reshape((1, dimension))\n",
    "    count = 0.\n",
    "    deficit=0\n",
    "    for i, tokenized_tweet in enumerate(tokenized_tweets):\n",
    "        for word in tokenized_tweet:\n",
    "            try:\n",
    "                vec += glove_model[word].reshape((1, dimension)) * tfidf[word]\n",
    "                count += 1.\n",
    "            except KeyError:\n",
    "                deficit+=1\n",
    "                continue\n",
    "        if count != 0:\n",
    "            vec /= count\n",
    "            \n",
    "        tweets_embeddings[i, :] = vec\n",
    "    \n",
    "    print \"deficit: \",deficit\n",
    "    return tweets_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "embedded_X_train=find_vector_representation_of_tweets(X_train,100,tfidf)\n",
    "embedded_X_test=find_vector_representation_of_tweets(X_test,100,tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_X_train[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_X_train = scale(embedded_X_train)\n",
    "embedded_X_test = scale(embedded_X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_X_train[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = list(range(embedded_X_train.shape[0]))\n",
    "idx2 = list(range(embedded_X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(embedded_X_train[idx,:], y_train[idx], epochs=10, batch_size=32, validation_data=(embedded_X_test[idx2,:], y_test[idx2]),verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
